---
title: "Assignment 1: Report"
author: "Andrea Plumbley"
---

## Abstract

A number of methods exist to manipulate text data in order to make predictions based on text. In this assignment a number of predictive models are built in order to 'predict the president', classifying which president said which sentences from SONA speeches between 1994 and 2022. There is a focus on how one can manipulate and structure text data to be used as input in predictive models. The different predictive models discussed here include Classification Trees, Boosted Trees, Standard Feed Forward Neural Networks and Convolutional Neural Networks. The best model for this specific problem was a Neural Network with one hidden layer which achieved classification accuracy of 57%. Overall the models performed poorly at classifying the South African presidents based on sentence input however the assignment highlights important aspects of working with text data and building predictive models.

## Introduction

The aim of this assignment is to 'predict the president', to build predictive models that take in a sentence of text from a speech and predict which South African president said it. The data set given is a collection of 36 State of the Nation Addresses (SONA) in South Africa, delivered between 1994 and 2022.

A number of techniques exist in order to use text data to make predictions. Different methods of structuring that data give rise to different ways in which models can be built and predictions made. Today, there is a large volume of text data to classify which can not be done manually and so techniques to do this using models need to be developed (Prasanna et al., 2018). This has led to much research in the field of text classification, with many finding that artifical neural networks give good classification results (Prasanna et al., 2018). More complex models such as convolutional neural networks and recurrent neural networks have also been shown to have good performance when classifying text data (Zhou et al. 2015).

The assignment focuses on text mining and manipulation as well as predictive models, particularly focusing on neural networks and their classification performance. Neural networks as considered due to the research suggesting that these models perform well at the task at hand. Classification trees and boosted trees are also used for prediction in order to compare the performance of the neural networks with other models and see if there are significant improvements.

First the data cleaning process and initial data exploration will be detailed and key features of the data discussed. The methods to format the data as well as the training, validation and test data splits will be outlined. The methods used to construct the different predictive models will then be detailed. Following this, results of the predictive models will be presented and discussed and final conclusions made.

```{r, echo=FALSE, messages=FALSE, warning=FALSE, include=FALSE}
## LOAD IN THE DATA
set.seed(2023)
# Required libraries
library(dplyr)
library(gghighlight)
library(tidyverse)
library(tidytext)
library(lubridate)
library(stringr)
library(rpart)
library(reticulate)
library(ggpubr)
library(tensorflow)
library(keras)
library(nnet)
library(ranger)

## Read the data in: code given by Ian  

# read in text data files and organise these into a data frame
filenames <- c('1994_post_elections_Mandela.txt', '1994_pre_elections_deKlerk.txt', '1995_Mandela.txt', '1996_Mandela.txt', '1997_Mandela.txt', '1998_Mandela.txt', 
               '1999_post_elections_Mandela.txt', '1999_pre_elections_Mandela.txt', '2000_Mbeki.txt', '2001_Mbeki.txt', '2002_Mbeki.txt', '2003_Mbeki.txt', 
               '2004_post_elections_Mbeki.txt', '2004_pre_elections_Mbeki.txt', '2005_Mbeki.txt', '2006_Mbeki.txt', '2007_Mbeki.txt', '2008_Mbeki.txt', 
               '2009_post_elections_Zuma.txt', '2009_pre_elections_ Motlanthe.txt', '2010_Zuma.txt', '2011_Zuma.txt', '2012_Zuma.txt', '2013_Zuma.txt', 
               '2014_post_elections_Zuma.txt', '2014_pre_elections_Zuma.txt', '2015_Zuma.txt', '2016_Zuma.txt', '2017_Zuma.txt', '2018_Ramaphosa.txt', 
               '2019_post_elections_Ramaphosa.txt', '2019_pre_elections_Ramaphosa.txt', '2020_Ramaphosa.txt', '2021_Ramaphosa.txt', '2022_Ramaphosa.txt', '2023_Ramaphosa.txt')


this_speech <- c()
this_speech[1] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/1994_post_elections_Mandela.txt', nchars = 27050)
this_speech[2] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/1994_pre_elections_deKlerk.txt', nchars = 12786)
this_speech[3] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/1995_Mandela.txt', nchars = 39019)
this_speech[4] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/1996_Mandela.txt', nchars = 39524)
this_speech[5] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/1997_Mandela.txt', nchars = 37489)
this_speech[6] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/1998_Mandela.txt', nchars = 45247)
this_speech[7] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/1999_post_elections_Mandela.txt', nchars = 34674)
this_speech[8] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/1999_pre_elections_Mandela.txt', nchars = 41225)
this_speech[9] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2000_Mbeki.txt', nchars = 37552)
this_speech[10] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2001_Mbeki.txt', nchars = 41719)
this_speech[11] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2002_Mbeki.txt', nchars = 50544)
this_speech[12] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2003_Mbeki.txt', nchars = 58284)
this_speech[13] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2004_post_elections_Mbeki.txt', nchars = 34590)
this_speech[14] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2004_pre_elections_Mbeki.txt', nchars = 39232)
this_speech[15] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2005_Mbeki.txt', nchars = 54635)
this_speech[16] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2006_Mbeki.txt', nchars = 48643)
this_speech[17] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2007_Mbeki.txt', nchars = 48641)
this_speech[18] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2008_Mbeki.txt', nchars = 44907)
this_speech[19] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2009_post_elections_Zuma.txt', nchars = 31101)
this_speech[20] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2009_pre_elections_Motlanthe.txt', nchars = 47157)
this_speech[21] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2010_Zuma.txt', nchars = 26384)
this_speech[22] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2011_Zuma.txt', nchars = 33281)
this_speech[23] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2012_Zuma.txt', nchars = 33376)
this_speech[24] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2013_Zuma.txt', nchars = 36006)
this_speech[25] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2014_post_elections_Zuma.txt', nchars = 29403)
this_speech[26] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2014_pre_elections_Zuma.txt', nchars = 36233)
this_speech[27] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2015_Zuma.txt', nchars = 32860)
this_speech[28] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2016_Zuma.txt', nchars = 32464)
this_speech[29] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2017_Zuma.txt', nchars = 35981)
this_speech[30] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2018_Ramaphosa.txt', nchars = 33290)
this_speech[31] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2019_post_elections_Ramaphosa.txt', nchars = 42112)
this_speech[32] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2019_pre_elections_Ramaphosa.txt', nchars = 56960)
this_speech[33] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2020_Ramaphosa.txt', nchars = 47910)
this_speech[34] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2021_Ramaphosa.txt', nchars = 43352)
this_speech[35] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2022_Ramaphosa.txt', nchars = 52972)
this_speech[36] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2022_Ramaphosa.txt', nchars = 52972)

sona <- data.frame(filename = filenames, speech = this_speech, stringsAsFactors = FALSE)

# extract year and president for each speech
sona$year <- str_sub(sona$filename, start = 1, end = 4)
sona$president_13 <- str_remove_all(str_extract(sona$filename, "[dA-Z].*\\."), "\\.")

# clean the sona dataset by adding the date and removing unnecessary text
replace_reg <- '(http.*?(\\s|.$))|(www.*?(\\s|.$))|&amp;|&lt;|&gt;|\n'

sona <-sona %>%
  mutate(speech = str_replace_all(speech, replace_reg , ' ')
         ,date = str_sub(speech, start=1, end=30)
         ,date = str_replace_all(date, "February", "02")
         ,date = str_replace_all(date, "June", "06")
         ,date = str_replace_all(date, "Feb", "02")
         ,date = str_replace_all(date, "May", "05")
         ,date = str_replace_all(date, "Jun", "06")
         ,date = str_replace_all(date, "Thursday, ","")
         ,date = str_replace_all(date, ' ', '-')        
         ,date = str_replace_all(date, "[A-z]",'')
         ,date = str_replace_all(date, '-----', '')
         ,date = str_replace_all(date, '----', '')
         ,date = str_replace_all(date, '---', '')
         ,date = str_replace_all(date, '--', '')
  )

```

## [Data Cleaning, Tokenization, and Exploration]{.underline}

The data that is used in this particular problem are the State of the Nation Addresses in South Africa from 1994 to 2022. There a 36 speeches given by 6 different presidents, namely Mandela, de Klerk, Mbkei, Zuma, Motlanthe and Ramaphosa. The initial steps in the solving the predict the president problem is to break the speeches up into their individual sentences and remove any unwanted characters, numbers or punctuation marks. The speeches were tokenized, broken up into smaller parts, using the unnest_tokens function in R. Each speech was split up into its sentences and the new data structure included the sentence along with the president who said it. For modelling purposes the sentences need to be only made up of their words so that when each sentence is tokenized into words, punctuation marks and numbers are not recognized as individual words. In order to do this the str_replace_all() function in the stringr package was used in order to remove any unwanted characters which were specified using a regular expression (regex). Number, commas, question marks and exclamation marks are some examples of the characters that were removed from analysis.

Some exploration is required to check how imbalanced the classes are in terms of how many sentences are linked with each president. Table 1 below indicates the number of sentences associated with each president. It is clear that de Klerk and Motlanthe have much fewer associated sentences, this is expected as each of them only delivered one speech. Because of this large discrepancy in number of sentences, de Klerk and Motlanthe are removed from the analysis. The remaining four classes are still imbalanced with Mandela having the fewest sentences with 1665. To account for this all classes are made to have 1665 sentences and this is done by sampling without replacement 1665 sentences from the remaining three presidents: Mbeki, Zuma and Ramaphosa.

[*Table 1: Number of sentences by each president.*]{.underline}

| de Klerk | Mandela | Mbeki | Motlanthe | Ramaphosa | Zuma |
|----------|---------|-------|-----------|-----------|------|
| 97       | 1665    | 2419  | 266       | 2286      | 2656 |

Having split the speeches up into sentences and removed the unwanted characters and balancing the number of sentences per president, each sentence could then be tokenized into its individual words. The sentences need to be broken down into words in order to create bag of words data structures which will be used in the predictive models.

```{r, echo=FALSE}
sona = sona[,2:5]

## Parse into sentences 
sona_sentences = unnest_tokens(sona, sentence, speech, token="sentences") %>%
  rename(president_name = president_13)

#some cleaning
replace_reg = '(https?:.*?(\\s|.$))|(www.*?(\\s|.$))|&amp;|&lt;|&gt|\\*|\\(|\\)|\\"|\\“|\\”|\\¦|\\[|\\]|\\>|\\¬|\\.|\\�'

sona_sentences_cleaned = sona_sentences %>% 
  mutate(sentences =str_replace_all(sona_sentences$sentence, replace_reg, ''))  %>%
  select(-sentence)

#sona_words = sona_sentences_cleaned %>% 
#  unnest_tokens(word, sentences, token = "words")
```

```{r, echo = FALSE, include = FALSE}
unique(sona_sentences_cleaned$president_name)
table(sona_sentences_cleaned$president_name)

sona_sentences_4 = sona_sentences_cleaned %>% 
  filter(president_name=="Mandela"|
           president_name=="Mbeki"|
           president_name=="Zuma"|
           president_name== "Ramaphosa")

set.seed(2023)
sona_sentences_4_balanced = sona_sentences_4 %>%
                              group_by(president_name) %>%
                              slice_sample(n=1665)  %>%
                              ungroup()
sona_sentences_FINAL = sona_sentences_4_balanced %>%
                        mutate(sentence_ID = row_number())
                              
table(sona_sentences_4_balanced$president_name)

```

Again the unnest_tokens() function is used to break the sentences up into words where each word is now associated with a president and sentence number in order to keep track of which words belong in which sentences and who said them.

Because many of the presidents all use common words throughout their speeches, these words need to be removed from the analysis because they do not help one to differentiate one president from another. These words are referred to as stop words and include words such as: the, and, if, them, etc. A stop_words dictionary exists which was used to remove these words from the analysis.

```{r, echo =FALSE, message=FALSE, warning=FALSE}

## Tokenize into words
sona_words = sona_sentences_FINAL %>% 
  unnest_tokens(word, sentences, token = "words")

all_words = sona_words %>%
  group_by(word) %>%
  count() %>%
  ungroup()

## Take out number or words with fewer than 3 letters
all_words_cleaned = all_words %>%
                    filter(!str_detect(word, "\\d")) %>%
                    filter(str_length(word) > 2)

## Remove stop words
stopwords = stop_words$word
all_words_final = anti_join(all_words_cleaned, stop_words, by = "word")

sentence_word_links = sona_words %>%
  inner_join(all_words_final) %>%
  group_by(sentence_ID, word) %>%
  count() %>%
  group_by(sentence_ID) %>%
  mutate(total = sum(n)) %>%
  ungroup()

bag_of_words = sentence_word_links %>% 
  select(sentence_ID, word, n) %>% 
  pivot_wider(names_from = word, values_from = n, values_fill = 0) %>%
  left_join(sona_sentences_FINAL %>% select(sentence_ID, president_name)) %>%
  select(sentence_ID, president_name, everything())

# Remove columns with all zeros
# zero_columns <- apply(bag_of_words, 2, function(col) all(col == 0))
# bag_of_words = bag_of_words[,!zero_columns]
```

We can consider the top words each presidents says as this will likely be used as one of the input data structures. Figure 1 displays the top 10 words said by each president. From this it is clear that there is significant overlap between presidents with words such as government, south and people being said by all presidents. There are however some words that are said many times by each president that do not appear in other presidents top 10 words. For example Zuma says compatriots many times and Ramaphosa refers frequently to the economy while others do not do so as frequently. These differences in the frequent words may be useful when deciding on the different data input structures to use in the predictive models.

```{r, echo = FALSE}

## Testing out using top 200 words said by each president.

each_president_common_words = sona_words %>%
                             group_by(president_name) %>%
                              count(word) %>%
                              ungroup()

## remove numbers here

common_words_remove_stop = anti_join(each_president_common_words, stop_words, by='word')

common_words_remove_stop_final = common_words_remove_stop %>%
                                  group_by(president_name) %>%
                                  filter(rank(desc(n)) <= 200) 

top_10_words = common_words_remove_stop_final %>%
                group_by(president_name) %>%
                filter(rank(desc(n)) <= 10) 

P1 = top_10_words %>%
  filter(president_name == "Mandela") %>%
  arrange(desc(n)) %>%
  mutate(id = 1:dim(.)[1]) %>%
  filter(id <= 15) %>%
  ggplot(aes(reorder(word, n), n)) + geom_col(fill = "blue", col = "black") + 
  coord_flip() + 
  xlab(" ") + ylab("Times word said") +
  theme_bw(base_size = 12) +
  gghighlight(id <= 5) + ggtitle("Mandela")

P2 = top_10_words %>%
  filter(president_name == "Mbeki") %>%
  arrange(desc(n)) %>%
  mutate(id = 1:dim(.)[1]) %>%
  filter(id <= 15) %>%
  ggplot(aes(reorder(word, n), n)) + geom_col(fill = "blue", col = "black") + 
  coord_flip() + 
  xlab(" ") + ylab("Times word said") +
  theme_bw(base_size = 12) +
  gghighlight(id <= 5) + ggtitle("Mbeki")

P3 = top_10_words %>%
  filter(president_name == "Zuma") %>%
  arrange(desc(n)) %>%
  mutate(id = 1:dim(.)[1]) %>%
  filter(id <= 15) %>%
  ggplot(aes(reorder(word, n), n)) + geom_col(fill = "blue", col = "black") + 
  coord_flip() + 
  xlab(" ") + ylab("Times word said") +
  theme_bw(base_size = 12) +
  gghighlight(id <= 5) + ggtitle("Zuma")

P4 = top_10_words %>%
  filter(president_name == "Ramaphosa") %>%
  arrange(desc(n)) %>%
  mutate(id = 1:dim(.)[1]) %>%
  filter(id <= 15) %>%
  ggplot(aes(reorder(word, n), n)) + geom_col(fill = "blue", col = "black") + 
  coord_flip() + 
  xlab(" ") + ylab("Times word said") +
  theme_bw(base_size = 12) +
  gghighlight(id <= 5) +ggtitle("Ramaphosa")


ggarrange(P1, P2, P3, P4, ncol = 2, nrow = 2)
```

[*Figure 1: Top 10 most common words said by Mandela, Mbeki, Zuma and Ramaphosa*]{.underline}

## [Data Input Structures]{.underline}

The text data needs to be put into a specific format for the predictive models for which we use a bag of words with counts and TFIDF values. In a bag of words model a document (in this case working with sentences) is represented by the set of words used in it. In this case each row of the bag of words data frame represents a sentence where the columns are the possible words used. The number of columns is very large because all words used by all presidents, excluding the stop words or other words 'cleaned away', are represented on the columns. The matrix is relatively sparse in that for each row (sentence) only a couple columns have values due to only a few words being used in each sentence. The values are counts of how many times each word was used in each sentence. This format is the first type of input data structure use in the predictive models. (In results tables this format is referred to as 'all BOW counts').

The second input structure used in the predictive models is similar in format to the above except that instead of the values being counts of each word in each sentence, the value is a TFIDF value. A TFIDF (Term-frequency-inverse-document-frequency) value is a way of distinguishing which words are more or less 'important' in the delivered speeches (Silge & Robinson, 2017) . This value is made up of two calculations:

-   The term frequency (tf) which is the number of times a specific terms appears in a document, divided by the number of terms in that document.

-   The inverse document frequency (idf) which is a measure of how many documents contain the specific terms and is calculated as the log of the total number of documents divided by the number of documents containing the specific word.

The TFIDF value for a word is the product of the tf and idf measures. This data structure thus contains all the words as columns, the sentences as the rows (made up of different words) and the values in each cell are the TFIDF values (Silge & Robinson, 2017). This data input structure may be more beneficial than the bag of words counts because it gives more information in terms of the relative importance or significance of the different words used in different sentences. (In results tables this format is referred to as 'All TFIDF').

An additional way of varying the input structure which also helps reduce the dimensions of input data is to use only the top 200 most frequent words used by each president. This will help see if using more frequently used words helps better identify which president is speaking. As seen in the initial data exploration of each presidents top 10 words, there may however be significant overlap in each presidents most frequently used words and so this may not be an effective way of improving the predictions. However there maybe be unique words that each president says often that help differentiate them from one another. The top 200 words format is similar to the bag of words format in that the columns are words that make up each presidents most repeated words and the rows represent the sentences. Both counts and TFIDF scores will be used in the 200 top words data input format. (In results tables these formats are referred to as '200 words counts' and '200 words TFIDF' respectively).

The above differing formats result in four different types of format of input data, two are what will be referred to as 'all words' formats which contain all the words except for those removed in cleaning. The one 'all words' format uses counts of word so form a bag of words model and the other 'all words' format uses TFIDF scores. The two '200 words' input formats refer to those inputs that use only the top 200 most repeated words from each president. Again, one of these uses word counts and the other uses TFIDF values.

```{r, echo = FALSE, warning=FALSE, message=FALSE}
ndocs <- length(unique(sentence_word_links$sentence_ID))

idf <- sentence_word_links%>% 
  group_by(word) %>% 
  summarize(docs_with_word = n()) %>% 
  ungroup() %>%
  mutate(idf = log(ndocs / docs_with_word)) %>% arrange(desc(idf))

sentence_tdf <- sentence_word_links%>% 
  left_join(idf, by = 'word') %>% 
  mutate(tf = n/total, tf_idf = tf * idf)

sentence_tdf <- sentence_tdf %>% 
  select(-idf, -tf, -tf_idf) %>% #remove the old ones we worked out
  bind_tf_idf(word, sentence_ID, n)   #replace with values from tidytext


tfidf <- sentence_tdf %>% 
  select(sentence_ID, word, tf_idf) %>%  # note the change, using tf-idf
  pivot_wider(names_from = word, values_from = tf_idf, values_fill = 0) %>%  
  left_join(sona_sentences_FINAL %>% select(sentence_ID,president_name)) %>%
  select(sentence_ID, president_name, everything())
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
## Testing out using top 200 words said by each president.

each_president_common_words = sona_words %>%
                             group_by(president_name) %>%
                              count(word, sort=TRUE) %>%
                              ungroup()

## remove numbers here

common_words_remove_stop = anti_join(each_president_common_words, stop_words, by='word')

common_words_remove_stop_final = common_words_remove_stop %>%
                                  group_by(president_name) %>%
                                  filter(rank(desc(n)) <= 200) 

sentence_word_links_common_words = sentence_word_links %>%
                                    filter(word %in% common_words_remove_stop_final$word)

#sort(unique(common_words_remove_stop_final$word))
#sort(unique(sentence_word_links_common_words$word))

## NEED SOME EDA PLOTS
bag_of_words_200 = sentence_word_links_common_words %>% 
  select(sentence_ID, word, n) %>% 
  pivot_wider(names_from = word, values_from = n, values_fill = 0) %>%
  left_join(sona_sentences_FINAL %>% select(sentence_ID, president_name)) %>%
  select(sentence_ID, president_name, everything())

## Do TFIDF with 200 words

ndocs_2 <- length(unique(sentence_word_links_common_words$sentence_ID))

idf_common <- sentence_word_links_common_words%>% 
  group_by(word) %>% 
  summarize(docs_with_word = n()) %>% 
  ungroup() %>%
  mutate(idf = log(ndocs_2 / docs_with_word)) %>% arrange(desc(idf))

sentence_tdf_common <- sentence_word_links_common_words%>% 
  left_join(idf, by = 'word') %>% 
  mutate(tf = n/total, tf_idf = tf * idf)

sentence_tdf_common <- sentence_tdf_common %>% 
  select(-idf, -tf, -tf_idf) %>% #remove the old ones we worked out
  bind_tf_idf(word, sentence_ID, n)   #replace with values from tidytext


# TREE with TFIDF 
## CHECK if deklerk and mothahle words are here 

tfidf_common <- sentence_tdf_common %>% 
  select(sentence_ID, word, tf_idf) %>%  # note the change, using tf-idf
  pivot_wider(names_from = word, values_from = tf_idf, values_fill = 0) %>%  
  left_join(sona_sentences_FINAL %>% select(sentence_ID,president_name)) %>%
  select(sentence_ID, president_name, everything())

```

## [Training, Validation and Test Splits]{.underline}

Having constructed the required data formats for the predictive model, the data must be split into training, validation and test sets before the predictive models are built. The split that will be used here is 60% training, 30% validation and 10% test data. The data splits are done grouping by president so that the classes of each president are balance in training , validation and test sets.

The seed was set to 2023 to ensure reproducibility. The training, validation and test sets were created by sampling from the sentence_ID variable and using this along with the anti_join() function to create the associated training, validation and test set vector of indices. For the different data formats, eg bag of words with counts versus TFIDF, the training data was created by selecting those rows where the sentence_ID variable matches those IDs in the training set vector of indices.

The training set will be used to build the different predictive models. The validation set will be used to select which hyper-parameters or model configuration to use. The test set will provide a final measure of performance of the selected model.

```{r, echo = FALSE, message=FALSE, warning=FALSE}
set.seed(2023)

sentences_ids = data.frame(sentence_ID = (bag_of_words$sentence_ID))
N = nrow(sentences_ids)

train_size = round(N*0.6)
val_size = round(N*0.3)
test_size = N - train_size - val_size

train_ids = data.frame(sentence_ID = sample(sentences_ids[,1], size = train_size, replace = FALSE))

remaining_ids = sentences_ids %>% anti_join(train_ids)

val_ids = data.frame(sentence_ID = sample(remaining_ids[,1], size = val_size, replace = FALSE))
test_ids = remaining_ids %>% anti_join(val_ids)

```

```{r, echo = FALSE}

## Create Training, Validation and Test Splits for the 4 data structures 

## BAG OF WORDS WITH COUNTS - ALL WORDS 
training_data_boW_all = bag_of_words %>%
                      right_join(train_ids, by='sentence_ID') %>%
                      select(-sentence_ID)
X_train_all_BAG = training_data_boW_all[,2:ncol(training_data_boW_all)]
Y_train_all_BAG = as.factor(training_data_boW_all$president_name)


val_data_boW_all = bag_of_words %>%
                  right_join(val_ids, by='sentence_ID') %>%
                  select(-sentence_ID)
X_val_all_BAG = val_data_boW_all[,2:ncol(val_data_boW_all)]
Y_val_all_BAG = as.factor(val_data_boW_all$president_name)

test_data_boW_all = bag_of_words %>%
                  right_join(test_ids, by='sentence_ID') %>%
                  select(-sentence_ID)
X_test_all_BAG = test_data_boW_all[,2:ncol(test_data_boW_all)]
Y_test_all_BAG = as.factor(test_data_boW_all$president_name)


## TFIDF - ALL WORDS 

training_data_tfidf_all = tfidf %>%
                      right_join(train_ids, by='sentence_ID') %>%
                      select(-sentence_ID)
X_train_all_tfidf = training_data_tfidf_all[,2:ncol(training_data_tfidf_all)]
Y_train_all_tfidf = as.factor(training_data_tfidf_all$president_name)


val_data_tfidf_all = tfidf %>%
                  right_join(val_ids, by='sentence_ID') %>%
                  select(-sentence_ID)
X_val_all_tfidf = val_data_tfidf_all[,2:ncol(val_data_tfidf_all)]
Y_val_all_tfidf = as.factor(val_data_tfidf_all$president_name)

test_data_tfidf_all = tfidf %>%
                  right_join(test_ids, by='sentence_ID') %>%
                  select(-sentence_ID)
X_test_all_tfidf = test_data_tfidf_all[,2:ncol(test_data_tfidf_all)]
Y_test_all_tfidf = as.factor(test_data_tfidf_all$president_name)


##  BAG OF WORDS WITH COUNTS - 200 Words

training_data_boW_200 = bag_of_words_200 %>%
                      right_join(train_ids, by='sentence_ID') %>%
                      select(-sentence_ID)
X_train_200_BAG = training_data_boW_200[,2:ncol(training_data_boW_200)]
Y_train_200_BAG = as.factor(training_data_boW_200$president_name)


val_data_boW_200 = bag_of_words_200 %>%
                  right_join(val_ids, by='sentence_ID') %>%
                  select(-sentence_ID)
X_val_200_BAG = val_data_boW_200[,2:ncol(val_data_boW_200)]
Y_val_200_BAG = as.factor(val_data_boW_200$president_name)

test_data_boW_200 = bag_of_words_200 %>%
                  right_join(test_ids, by='sentence_ID') %>%
                  select(-sentence_ID)
X_test_200_BAG = test_data_boW_200[,2:ncol(test_data_boW_200)]
Y_test_200_BAG = as.factor(test_data_boW_200$president_name)


## TFIDF - 200 WORDS

training_data_tfidf_200 = tfidf_common %>%
                      right_join(train_ids, by='sentence_ID') %>%
                      select(-sentence_ID)
X_train_200_tfidf = training_data_tfidf_200[,2:ncol(training_data_tfidf_200)]
Y_train_200_tfidf = as.factor(training_data_tfidf_200$president_name)


val_data_tfidf_200 = tfidf_common %>%
                  right_join(val_ids, by='sentence_ID') %>%
                  select(-sentence_ID)
X_val_200_tfidf = val_data_tfidf_200[,2:ncol(val_data_tfidf_200)]
Y_val_200_tfidf = as.factor(val_data_tfidf_200$president_name)

test_data_tfidf_200 = tfidf_common %>%
                  right_join(test_ids, by='sentence_ID') %>%
                  select(-sentence_ID)
X_test_200_tfidf = test_data_tfidf_200[,2:ncol(test_data_tfidf_200)]
Y_test_200_tfidf = as.factor(test_data_tfidf_200$president_name)


```

## [Methods]{.underline}

Having outlined the various data input structures that will be considered in the models as well as how the data has been split into its training, validation and testing classes, the predictive models that are used are now outlined and methods to implement them explained.

### Classification Tree

The first predictive model applied to the data is a simple classification tree. A classification tree is a decision tree that parititions the data on different variables in order to categorize each observation into a class. In this case a classification tree would take a sentence and based on the word counts or TFIDF counts, classify the sentence as being from one of the four presidents.

A classification tree was fit using the rpart() function in R. Four models were fit, one on each of the four training data sets which are the full bag-of-words counts data, the full TFIDF data, the top 200 bag of words count data and the top 200 TFIDF data. The 'method' argument was set to 'class' because the response is categorical. The remaining arguments of the rpart() function were left at the default settings. For the classification tree model, no tuning was performed.

### Boosting - Tree Based Method

The second predictive model that was fit was boosted trees. This is an ensemble learning method and follows on from a simple classification tree. The boosting algorithm iteratively builds decision trees where each subsequent tree learns from previous trees mistakes and so the model gradually improves (Hastie, Tibshirani & Friedman, 2008).

To fit the boosted tree the gbm() function from the gbm package was used in R. The formula was the same as in the case of the classfication tree, predicting the president based on the bag of words and TFIDF data. For the input data structures that used only the top 200 words of each president, the number of trees was set to 500. However the input data structure with all words was much larger and took much longer to fit and so only 10 trees were used for this large data set. The interaction.depth was set to 2, shrinkage to 0.01 and bag.fraction to 1 for all models. The remaining parameters were left at the default settings.

### Feed-Forward Neural Network

A standard feed-forward neural network was fit to the different data types to predict the president who delivered the sentence. Here a number of different configurations of neural networks, with different numbers of layers and nodes were constructed and the validation set used to select the best model.

There are many architectures, or configurations, that can be done by varying the number of layers in the network, number of nodes per layers and other parameters such as learning rate and activation functions. The selection or 'tuning' of these parameters is done manually by fitting 3 different neural networks using the training data and selecting the optimal architecture based on the validation set performance. The differing configurations will be fit on the four different input data structures.

The keras package was used to train and fit the neural networks, making use of the keras_model_sequential(), compile() and fit() functions as well as various functions used to specify hidden layers, output layers and dropout.

There were a number of parameters that were common across all 3 network architectures. The output layer for all networks had 4 nodes, corresponding to the one-hot encoding of the 4 presidents. The activation function on the output layer was set to "softmax" due to the categorical nature of the response variable. The "adam" optimizer was used and categorical crossentropy specified as the error measure. In terms of fitting the model with the fit() function in keras, 30 epochs over the data were done and the batch size was set to 200 observations.

The differences in the configurations considered were the following:

**Neural Network 1:** The first neural network was the 'simplest' model out of the three. This model had 1 hidden layer containing 50 nodes. The "Relu" activation function was specified on the hidden layer.

**Neural Network 2:** The second network contained 3 hidden layers with 50, 10 and 5 nodes per layer respectively. "tanh" activation functions were used on each of these hidden layers.

**Neural Network 3:** The third network contained 2 hidden layers with a dropout layer between the 2 hidden layers. The first layer contained 500 nodes while the second hidden layer contained 20 nodes. The dropout layer was between these two layers and a rate of 0.25 was specified.

These models were fit to the data input structures with all words, which had an input argument in the first layer specified at 9085 because there were 9085 columns in the bag of words data input structure. When fitting these models to the bag of words for only the top 200 words per president this input argument for the first layer was set to 378, the number of columns in that data input structure.

### Convolutional Neural Network

The final predictive model considered for this problem is a convolutional neural network. A convolutional neural network is a more advanced model than a standard neural network and makes use of convolution and pooling isteps n order to fit the model. This type of model is usually suited to problems such as image classification as the convolutional layers make use of filters which can 'summarize' the data in that way extract key features.

The convolutional neural network required a slightly different input format to the four data structure formats used above. For the CNNs, word embeddings are used to represent the speech sentences more directly, instead of using the bag of words format as before. Word embeddings are a way of representing words as numbers which are then fed into the model. This is done using the text_tokenizer() and texts_to_sequences() functions in R which populate the new data in terms of the embeddings. The maximum length is set to 15 which makes all sentence of length 15 and pads those sentences that are too short using the pad_sequences() function. Once sentences have been transformed in this way they can be used in the CNN model. Hence for this predictive model only one format of input data is being tested. The training, validation and test data sets still represent the same sentences as before but embedding makes them look different.

Two different architectures of convolutional neural network were fitted to the data in order to make predictions. The first layer of both networks is an embedding layer which is required as a result of the word embedding for the data. The last layer, as in the case of the feed forward neural network, is a layer with 4 nodes and a "softmax" activation function. The remaining network specifications are as follows:

**CNN 1:** Following the first layer a dropout layer is specified with a rate of 0.2. A convolutional layer is then specified with 64 filters with kernel_size set to 8 and "relu" activation used. A pooling layer with a pool_size of 2 is then specified. After flattening, a 'normal' hidden layer with 300 nodes and "relu" activation is specified.

**CCN 2:** Following the first layer a dropout layer with a rate of 0.1 is specified. A convolutional layer with 36 filters and a kernel_size set to 6 is then specified, with a "tanh" activation function. The pooling layer is the same as CCN 1 with a pool_size of 2. The final hidden layer, after flattening, has 100 hidden nodes and a "tanh" activation function.

## [Results]{.underline}

Having outlined the methods and models implemented, the results of these models are now given. For each of the models, where applicable, the classification accuracy of the models are given in terms of the validation data set. If different configurations of the models have been presented the model with the best validation accuracy is selected and the test set then applied to this model to obtain a final measure of classification accuracy.

### Classification Tree

The results of the classification tree fit to the four different data input structures is given in Table 2 below. This table specifies the classification accuracy for both training and test sets. The validation set was not used here because no tuning was performed so no 'best' model was required to be selected. From the results it is clear that the classification tree does very poorly at classifying which president said each sentence with the model only being a slight improvement from one just randomly choosing a president (25%). The classification tree performs badly on all four data input structures and for this model the different input structures to do not improve the accuracy significantly.

```{r, echo = FALSE}

## Classification Tree Results:

## CT 1 - All Words - BoW Counts

training_all_BAG = cbind(Y_train_all_BAG, X_train_all_BAG)
fit_all_BAG <- rpart(Y_train_all_BAG ~ ., training_all_BAG, method = 'class')

#plot(fit_all_BAG, main = 'Full Classification Tree')
#text(fit_all_BAG, use.n = TRUE, all = TRUE, cex=.8)

fittedtrain <- predict(fit_all_BAG, type = 'class')
predtrain <- table(training_all_BAG$Y_train_all_BAG, fittedtrain)
#predtrain
accuracy_all_BAG_train = round(sum(diag(predtrain))/sum(predtrain), 3) # training accuracy

fittedtest <- predict(fit_all_BAG, newdata = cbind(Y_test_all_BAG, X_test_all_BAG), type = 'class')
predtest <- table(Y_test_all_BAG, fittedtest)
#predtest
accuracy_all_BAG_test = round(sum(diag(predtest))/sum(predtest), 3) # test accuracy 


## CT 2 - All Words - TFIDF

training_all_TFIDF = cbind(Y_train_all_tfidf, X_train_all_tfidf)
fit_all_TFIDF <- rpart(Y_train_all_tfidf ~ ., training_all_TFIDF, method = 'class')

#plot(fit_all_BAG, main = 'Full Classification Tree')
#text(fit_all_BAG, use.n = TRUE, all = TRUE, cex=.8)

fittedtrain <- predict(fit_all_TFIDF, type = 'class')
predtrain <- table(training_all_TFIDF$Y_train_all_tfidf, fittedtrain)
#predtrain
accuracy_all_TFIDF_train = round(sum(diag(predtrain))/sum(predtrain), 3) # training accuracy

fittedtest <- predict(fit_all_TFIDF, newdata = cbind(Y_test_all_tfidf, X_test_all_tfidf), type = 'class')
predtest <- table(Y_test_all_tfidf, fittedtest)
#predtest
accuracy_all_TFIDF_test = round(sum(diag(predtest))/sum(predtest), 3) # test accuracy 

## CT 3 - 200 Words - BoW Counts

training_200_BAG = cbind(Y_train_200_BAG, X_train_200_BAG)
fit_200_BAG <- rpart(training_200_BAG$Y_train_200_BAG ~ ., data =  training_200_BAG, method = 'class')

#plot(fit_200_BAG, main = 'Full Classification Tree')
#text(fit_200_BAG, use.n = TRUE, all = TRUE, cex=.8)

fittedtrain <- predict(fit_200_BAG, newdata = cbind(Y_train_200_BAG, X_train_200_BAG), type = 'class')
predtrain <- table(training_200_BAG$Y_train_200_BAG, fittedtrain)
##predtrain
accuracy_200_BAG_train = round(sum(diag(predtrain))/sum(predtrain), 3) # training accuracy

fittedtest <- predict(fit_200_BAG, newdata = cbind(Y_test_200_BAG, X_test_200_BAG), type = 'class')
predtest <- table(Y_test_200_BAG, fittedtest)
#predtest
accuracy_200_BAG_test = round(sum(diag(predtest))/sum(predtest), 3) # test accuracy 


## CT 4 - 200 Words - TFIDF

training_200_TFIDF = cbind(Y_train_200_tfidf, X_train_200_tfidf)
fit_200_TFIDF <- rpart(Y_train_200_tfidf ~ ., training_200_TFIDF, method = 'class')

#plot(fit_200_TFIDF, main = 'Full Classification Tree')
#text(fit_200_TFIDF, use.n = TRUE, all = TRUE, cex=.8)

fittedtrain <- predict(fit_200_TFIDF, newdata = cbind(Y_train_200_tfidf, X_train_200_tfidf), type = 'class')
predtrain <- table(training_200_TFIDF$Y_train_200_tfidf, fittedtrain)
#predtrain
accuracy_200_TFIDF_train = round(sum(diag(predtrain))/sum(predtrain), 3) # training accuracy

fittedtest <- predict(fit_200_TFIDF, newdata = cbind(Y_test_all_tfidf, X_test_all_tfidf), type = 'class')
predtest <- table(Y_test_200_tfidf, fittedtest)
#predtest
accuracy_200_TFIDF_test = round(sum(diag(predtest))/sum(predtest), 3) # test accuracy 
```

[*Table 2: Classification Accuracy of Simple Classification Tree*]{.underline}

| Data Input Structure             | All BOW Counts             | All TFIDF                    | 200 Words Counts           | 200 Words TFIDF              |
|---------------|---------------|---------------|---------------|---------------|
| Training Classification Accuracy | `r accuracy_all_BAG_train` | `r accuracy_all_TFIDF_train` | `r accuracy_200_BAG_train` | `r accuracy_200_TFIDF_train` |
| Test Classification Accuracy     | `r accuracy_all_BAG_test`  | `r accuracy_all_TFIDF_test`  | `r accuracy_200_BAG_test`  | `r accuracy_200_TFIDF_test`  |

### Boosted Trees

The boosted tree model was also applied to all four data input structures as above, however the model for the larger data input structures was varied slight for computational reasons. Table 3 below gives the validation data set classification accuracy for the four different boosted tree models.

```{r, echo = FALSE, warning=FALSE, message=FALSE}
library(gbm)
library(caret)

## Gradient Boosting using gbm
## 200 _ BAG

data1 = as.data.frame(cbind(Y_train_200_BAG, X_train_200_BAG))
data1 = na.omit(data1)
gbm_200_BAG <- gbm(as.factor(data1$Y_train_200_BAG) ~ ., data = data1, 
                     n.trees = 500, 
                     distribution = "multinomial",
                     interaction.depth = 2, 
                     shrinkage = 0.01, 
                     bag.fraction = 1, 
                     verbose = F)
gbm_pred <- predict(gbm_200_BAG, as.data.frame(X_val_200_BAG))
pred = apply(gbm_pred, 1, which.max)

p = table(pred,  as.numeric(Y_val_200_BAG))
accuracy_200_BAG = round(sum(diag(p))/sum(p), 3)

## 200 _ TFIDF

data2 = as.data.frame(cbind(Y_train_200_tfidf, X_train_200_tfidf))
data2 = na.omit(data2)
gbm_200_tfidf <- gbm(as.factor(data2$Y_train_200_tfidf) ~ ., data = data2, 
                     n.trees = 500, 
                     distribution = "multinomial",
                     interaction.depth = 2, 
                     shrinkage = 0.01, 
                     bag.fraction = 1,
                     verbose = F)
gbm_pred <- predict(gbm_200_tfidf, as.data.frame(X_val_200_tfidf))
pred = apply(gbm_pred, 1, which.max)

p = table(pred,  as.numeric(Y_val_200_tfidf))
accuracy_200_tfidf = round(sum(diag(p))/sum(p), 3)

## All - BAG

data3 = as.data.frame(cbind(Y_train_all_BAG, X_train_all_BAG))
data3 = na.omit(data3)
gbm_all_BAG <- gbm(as.factor(data3$Y_train_all_BAG) ~ ., data = data3, 
                     n.trees = 10, 
                     distribution = "multinomial",
                     interaction.depth = 2, 
                     shrinkage = 0.01, 
                     bag.fraction = 1, 
                     verbose = F)
gbm_pred <- predict(gbm_all_BAG, as.data.frame(X_val_all_BAG))
pred = apply(gbm_pred, 1, which.max)

p = table(pred,  as.numeric(Y_val_200_BAG))
accuracy_all_BAG = round(sum(diag(p))/sum(p), 3)

## ALL - TFIDF

data4 = as.data.frame(cbind(Y_train_all_tfidf, X_train_all_tfidf))
data4 = na.omit(data4)
gbm_all_tfidf <- gbm(as.factor(data4$Y_train_all_tfidf) ~ ., data = data4, 
                     n.trees = 10, 
                     distribution = "multinomial",
                     interaction.depth = 2, 
                     shrinkage = 0.01, 
                     bag.fraction = 1, 
                     verbose = F)
gbm_pred <- predict(gbm_all_tfidf, as.data.frame(X_val_all_tfidf))
pred = apply(gbm_pred, 1, which.max)

p = table(pred,  as.numeric(Y_val_all_tfidf))
accuracy_all_tfidf = round(sum(diag(p))/sum(p), 3)






```

[*Table 3: Classification Accuracy of Boosted Trees*]{.underline}

| Data Input Structure               | All BOW Counts       | All TFIDF              | 200 Words Counts     | 200 Words TFIDF        |
|---------------|---------------|---------------|---------------|---------------|
| Validation Classification Accuracy | `r accuracy_all_BAG` | `r accuracy_all_tfidf` | `r accuracy_200_BAG` | `r accuracy_200_tfidf` |

From the above table it is clear that the models that made use of the most common words (top 200 from each president) performed significantly better than those that use all words. This however was expected because the number of trees used in these models was much much larger which gave the model more tree to learn from. While the model using all words did not perform particularly well, with a classification accuracy of around on 30%, this is an improvement from the simple classification tree and is slightly better than the naive approach of guessing a president (25%).

```{r, echo = FALSE, warning=FALSE, message=FALSE}
gbm_pred <- predict(gbm_200_tfidf, as.data.frame(X_test_200_tfidf))
pred = apply(gbm_pred, 1, which.max)

p = table(pred,  as.numeric(Y_test_200_tfidf))
accuracy_final = round(sum(diag(p))/sum(p), 3)
```

The best performing boosted tree model of the four is the top 200 words per president in the TFIDF format, with a classification accuracy of `r accuracy_200_tfidf`. Applying the test set to this model, one obtains a classification accuracy of `r accuracy_final`. This is the final classification accuracy from the best model of the boosted trees predictive model. While this is not a good classification accuracy, it is better than the naive model by around 20%.

### Feed-Forward Neural Network

The third predictive model used for this problem was the standard feed-forward neural network. As discussed in the methods section, three different architectures of networks were tested here on the four different data input structures. Table 4 below gives the validation data set classification accuracy for the different network architectures for the different data input structures.

```{r, echo = FALSE, message=FALSE, warning=FALSE}

Y_train_all_tfidf = as.factor(Y_train_all_tfidf)
Y_val_all_tfidf = as.factor(Y_val_all_tfidf)
Y_test_all_tfidf = as.factor(Y_test_all_tfidf)

# Make sure X is in matrix format 
X_train_200_BAG = as.matrix(X_train_200_BAG)
X_train_200_tfidf = as.matrix(X_train_200_tfidf)
X_train_all_BAG = as.matrix(X_train_all_BAG)
X_train_all_tfidf = as.matrix(X_train_all_tfidf)

X_val_200_BAG = as.matrix(X_val_200_BAG)
X_val_200_tfidf = as.matrix(X_val_200_tfidf)
X_val_all_BAG = as.matrix(X_val_all_BAG)
X_val_all_tfidf = as.matrix(X_val_all_tfidf)

X_test_200_BAG = as.matrix(X_test_200_BAG)
X_test_200_tfidf = as.matrix(X_test_200_tfidf)
X_test_all_BAG = as.matrix(X_test_all_BAG)
X_test_all_tfidf = as.matrix(X_test_all_tfidf)

#Keras takes in one-hot encoding

Y_train =  to_categorical(as.integer(unlist(Y_train_all_tfidf)) - 1)


Y_val = to_categorical(as.integer(unlist(Y_val_all_tfidf)) - 1)


Y_test =  to_categorical(as.integer(unlist(Y_test_all_tfidf)) - 1)


## ========= ALL WORDS MODELS =====================================================
## MODEL 1 - SIMPLE MODEL:
#- 1 Hidden Layer 
#- 50 Nodes on Hidden Layer 
#- Relu activation on hidden layer

model1 <- keras_model_sequential() %>%
  layer_dense(units = 50, input_shape = c(9085), activation = "relu") %>%
  layer_dense(units = 4, activation = "softmax")
#summary(model1)
model1 %>% compile(
  loss = "categorical_crossentropy",
  optimizer = "adam",
  metrics = "accuracy"
)
history1.1 <- model1 %>% fit(X_train_all_tfidf, Y_train, epochs = 30, batch_size = 200, verbose = 0) 
#plot(history1.1)
results1.1 <- model1 %>% evaluate(X_val_all_tfidf, Y_val, batch_size=200, verbose = 0)

history1.2 <- model1 %>% fit(X_train_all_BAG, Y_train, epochs = 30, batch_size = 200, verbose = 0) 
#plot(history1.2)
results1.2 <- model1 %>% evaluate(X_val_all_BAG, Y_val, batch_size=200, verbose = 0)


## MODEL 2 - Slightly more complicated MODEL:
#- 3 Hidden Layer 
#- 50-10-5 Nodes on Hidden Layer 
#- Relu activation on hidden layers

model2 <- keras_model_sequential() %>%
  layer_dense(units = 50, input_shape = c(9085), activation = "tanh") %>%
  layer_dense(units = 10, input_shape = c(50), activation = "tanh") %>%
  layer_dense(units = 5, input_shape = c(10), activation = "tanh") %>%
  layer_dense(units = 4, activation = "softmax")
#summary(model2)
model2 %>% compile(
  loss = "categorical_crossentropy",
  optimizer = "adam",
  metrics = "accuracy"
)
history2.1 <- model2 %>% fit(X_train_all_tfidf, Y_train, epochs = 30, batch_size = 200, verbose = 0) 
#plot(history2.1)
results2.1 <- model2 %>% evaluate(X_val_all_tfidf, Y_val, batch_size=200, verbose = 0)

history2.2 <- model2 %>% fit(X_train_all_BAG, Y_train, epochs = 30, batch_size = 200, verbose = 0) 
#plot(history2.1)
results2.2 <- model2 %>% evaluate(X_val_all_BAG, Y_val, batch_size=200, verbose = 0)

## MODEL 3 -  2 hidden layers with dropout layer in between :
#- 3 Hidden Layer 
#- 500-20 Nodes on Hidden Layer 
#- Relu activation on hidden layers
#- dropout layer with 0.25

model3 <- keras_model_sequential() %>%
  layer_dense(units = 500, input_shape = c(9085), activation = "relu") %>%
  layer_dropout(rate=0.25) %>%
  layer_dense(units = 20, input_shape = c(500), activation = "relu") %>%
  layer_dense(units = 4, activation = "softmax")
#summary(model3)
model3 %>% compile(
  loss = "categorical_crossentropy",
  optimizer = "adam",
  metrics = "accuracy"
)
history3.1 <- model3 %>% fit(X_train_all_tfidf, Y_train, epochs = 30, batch_size = 200, verbose = 0) 
#plot(history3.1)
results3.1 <- model3 %>% evaluate(X_val_all_tfidf, Y_val, batch_size=200, verbose = 0)
history3.2 <- model3 %>% fit(X_train_all_BAG, Y_train, epochs = 30, batch_size = 200, verbose = 0) 
#plot(history3.2)
results3.2 <- model3 %>% evaluate(X_val_all_BAG, Y_val, batch_size=200, verbose = 0)
## =====================================================================================


## ======================= 200 Words Models =================================
## MODEL 1 - SIMPLE MODEL:
#- 1 Hidden Layer 
#- 50 Nodes on Hidden Layer 
#- Relu activation on hidden layer

## Figure out which rows contain NA's

rows_with_na = which(apply(X_train_200_BAG, 1, function(row) any(is.na(row))))
rows_with_na = which(apply(X_train_200_tfidf, 1, function(row) any(is.na(row))))

X_train_new_200_TFIDF = X_train_200_tfidf[-rows_with_na,]
X_train_new_200_BAG = X_train_200_BAG[-rows_with_na,]
Y_train_new = Y_train[-rows_with_na,]

model1 <- keras_model_sequential() %>%
  layer_dense(units = 50, input_shape = c(378), activation = "tanh") %>%
  layer_dense(units = 4, activation = "softmax")
#summary(model1)
model1 %>% compile(
  loss = "categorical_crossentropy",
  optimizer = "adam",
  metrics = "accuracy"
)
history1.1_200 <- model1 %>% fit(X_train_new_200_TFIDF, Y_train_new, epochs = 30, batch_size = 200, verbose = 0) 
#plot(history1.1_200)
results1.1_200 <- model1 %>% evaluate(X_val_200_tfidf, Y_val, batch_size=200, verbose = 0)

history1.2_200 <- model1 %>% fit(X_train_new_200_BAG, Y_train_new, epochs = 30, batch_size = 200, verbose = 0) 
#plot(history1.2_200)
results1.2_200 <- model1 %>% evaluate(X_val_200_BAG, Y_val, batch_size=200, verbose = 0)


## MODEL 2 - Slightly more complicated MODEL:
#- 3 Hidden Layer 
#- 50-10-5 Nodes on Hidden Layer 
#- Relu activation on hidden layers

model2 <- keras_model_sequential() %>%
  layer_dense(units = 50, input_shape = c(378), activation = "tanh") %>%
  layer_dense(units = 10, input_shape = c(50), activation = "tanh") %>%
  layer_dense(units = 5, input_shape = c(10), activation = "tanh") %>%
  layer_dense(units = 4, activation = "softmax")
#summary(model2)
model2 %>% compile(
  loss = "categorical_crossentropy",
  optimizer = "adam",
  metrics = "accuracy"
)
history2.1_200 <- model2 %>% fit(X_train_new_200_TFIDF, Y_train_new, epochs = 30, batch_size = 200, verbose = 0) 
#plot(history2.1_200)
results2.1_200 <- model2 %>% evaluate(X_val_200_tfidf, Y_val, batch_size=200, verbose = 0)

history2.2_200 <- model2 %>% fit(X_train_new_200_BAG, Y_train_new, epochs = 30, batch_size = 200, verbose = 0) 
#plot(history2.2_200)
results2.2_200 <- model2 %>% evaluate(X_val_200_BAG, Y_val, batch_size=200, verbose = 0)

## MODEL 3 -  2 hidden layers with dropout layer in between :
#- 3 Hidden Layer 
#- 500-20 Nodes on Hidden Layer 
#- Relu activation on hidden layers
#- dropout layer with 0.25

model3 <- keras_model_sequential() %>%
  layer_dense(units = 500, input_shape = c(378), activation = "relu") %>%
  layer_dropout(rate=0.25) %>%
  layer_dense(units = 20, input_shape = c(500), activation = "relu") %>%
  layer_dense(units = 4, activation = "softmax")
#summary(model3)
model3 %>% compile(
  loss = "categorical_crossentropy",
  optimizer = "adam",
  metrics = "accuracy"
)
history3.1_200 <- model3 %>% fit(X_train_new_200_TFIDF, Y_train_new, epochs = 30, batch_size = 200, verbose = 0) 
#plot(history3.1_200)
results3.1_200 <- model3 %>% evaluate(X_val_200_tfidf, Y_val, batch_size=200, verbose = 0)
history3.2_200 <- model3 %>% fit(X_train_new_200_BAG, Y_train_new, epochs = 30, batch_size = 200, verbose = 0) 
#plot(history3.2_200)
results3.2_200 <- model3 %>% evaluate(X_val_200_tfidf, Y_val, batch_size=200, verbose = 0)


## ==========================================================================

```

[*Table 4: Validation Set Classification Accuracy of Neural Networks*]{.underline}

| Data Input Structure                   | All BOW Counts                         | All TFIDF                              | 200 Words Counts                           | 200 Words TFIDF                            |
|---------------|---------------|---------------|---------------|---------------|
| NN1 Validation Classification Accuracy | `r round(as.matrix(results1.2)[2], 3)` | `r round(as.matrix(results1.1)[2], 3)` | `r round(as.matrix(results1.2_200)[2],3)`  | `r round(as.matrix(results1.1_200)[2], 3)` |
| NN2 Validation Classification Accuracy | `r round(as.matrix(results2.2)[2], 3)` | `r round(as.matrix(results2.1)[2], 3)` | `r round(as.matrix(results2.2_200)[2], 3)` | `r round(as.matrix(results2.1_200)[2], 3)` |
| NN3 Validation Classification Accuracy | `r round(as.matrix(results3.2)[2], 3)` | `r round(as.matrix(results3.1)[2], 3)` | `r round(as.matrix(results3.2_200)[2], 3)` | `r round(as.matrix(results3.1_200)[2], 3)` |

An improvement is seen for these neural network classification models from the boosted tree models with the models using all words having over 50% classification accuracy. In general the neural networks that use all words perform better at classifying the presidents than the models which use only the top 200 words used by each president. Comparing both input structure that use all words, it can be seen that the TFIDF input structure performs slightly better than the standard counts bag of words input data structure. In terms of the three different network architectures the best network architecture depends on the input data structure used. For the BOW counts data, the NN1 and NN3 have almost the same accuracy with NN2 slightly lower. For the TFIDF data, the simplest neural network performs the best with a classification accuracy of `r round(as.matrix(results1.1)[2], 3)`. The model NN1, with TFIDF data has the highest validation set accuracy and so is selected as the best model.

```{r, echo=FALSE}
model1 <- keras_model_sequential() %>%
  layer_dense(units = 50, input_shape = c(9085), activation = "relu") %>%
  layer_dense(units = 4, activation = "softmax")
#summary(model1)
model1 %>% compile(
  loss = "categorical_crossentropy",
  optimizer = "adam",
  metrics = "accuracy"
)
history1.1 <- model1 %>% fit(X_train_all_tfidf, Y_train, epochs = 30, batch_size = 200, verbose = 0) 
#plot(history1.1)
results_test <- model1 %>% evaluate(X_test_all_tfidf, Y_test, batch_size=200, verbose = 0)

```

The test data set is applied to this model to obtain a final prediction accuracy. The classification accuracy on the test data, using TFIDF input and NN1 is `r round(as.matrix(results_test)[2], 3)` . While this is not a very good classification accuracy it is much better than the naive classification approach (25%) and is fairly good in terms of the accuracy seen in other models so far.

### Convolutional Neural Network

The final predictive method that was considered for the predict-the-president problem was a convolutional neural network. As already discussed in the methods section, this predictive model took in a different format of the data by using word embeddings for each of the sentences instead of the bag of words model of input data.

Figure 2 below gives a histogram of the sentence lengths using the embeddings, a max length of 15 was selected as the majority of sentences are less than 15 words with some substantial outlying sentences being very long. Padding was applied to those sentences which did not have enough words.

```{r, echo = FALSE, warning=FALSE, message=FALSE}
max_features <- 1000        # choose max_features most popular words
tokenizer = text_tokenizer(num_words = max_features)
fit_text_tokenizer(tokenizer, sona_sentences_FINAL$sentences[23])


#sona_sentences_FINAL$sentences[23]
tts <- texts_to_sequences(tokenizer, sona_sentences_FINAL$sentences[23])
#tts

#tokenizer$word_index[tts[[1]]]

sequences = tokenizer$texts_to_sequences(sona_sentences_FINAL$sentences)

train_row = train_ids$sentence_ID
val_row = val_ids$sentence_ID
test_row = test_ids$sentence_ID

y = as.factor(sona_sentences_FINAL$president_name)
y = as.integer(y)

train <- list()
val <- list()
test = list()
train$x <- sequences[train_ids$sentence_ID]
val$x <-  sequences[val_ids$sentence_ID]
test$x = sequences[test_ids$sentence_ID]

train$y <- y[train_row]
val$y <-  y[val_row]
test$y = y[test_row]

hist(unlist(lapply(sequences, length)), breaks=20, xlab = "Sentence Length", main = "")

maxlen <- 15            
x_train <- train$x %>% pad_sequences(maxlen = maxlen)
x_val <- val$x %>% pad_sequences(maxlen = maxlen)
x_test = test$x %>% pad_sequences(maxlen=maxlen)

embedding_dims <- 10
model <- keras_model_sequential() %>% 
  layer_embedding(max_features, output_dim = embedding_dims, input_length = maxlen) %>%
  layer_dropout(0.2) %>%
  layer_conv_1d(filters = 64, kernel_size = 8, activation = "relu") %>%
  layer_max_pooling_1d(pool_size = 2) %>%
  layer_flatten() %>%
  layer_dense(300, activation = "relu") %>%
  layer_dense(4, activation = "softmax")

#summary(model)

model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = "adam",
  metrics = "accuracy"
)

history <- model %>% fit(x_train,Y_train,
    batch_size = 200, epochs = 30, verbose = 0)
#plot(history)
results_CNN1 <- model %>% evaluate(x_val, Y_val, batch_size=200, verbose = 0)

## Need to fix this
embedding_dims <- 10
model <- keras_model_sequential() %>% 
  layer_embedding(max_features, output_dim = embedding_dims, input_length = maxlen) %>%
  layer_dropout(0.1) %>%
  layer_conv_1d(filters = 36, kernel_size = 6, activation = "tanh") %>%
  layer_max_pooling_1d(pool_size = 2) %>%
  layer_flatten() %>%
  layer_dense(100, activation = "tanh") %>%
  layer_dense(4, activation = "softmax")

#summary(model)

model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = "adam",
  metrics = "accuracy"
)

history <- model %>% fit(x_train,Y_train,
    batch_size = 200, epochs = 30, verbose = 0)
#plot(history)

results_CNN2 <- model %>% evaluate(x_val, Y_val, batch_size=200, verbose = 0)
result_test_CNN = model %>% evaluate(x_test, Y_test, batch_size=200, verbose = 0)

```

[*Figure 2: Sentence length with embeddings*]{.underline}

Two different architectures were used here as outlined in the methods. Table 5 below summarizes the results of the two methods.

[*Table 5: Validation Set Classification Accuracy of CNN*]{.underline}

|                                    | CNN 1                                    | CNN 2                                    |
|------------------------|------------------------|------------------------|
| Validation Classification Accuracy | `r round(as.matrix(results_CNN1)[2], 3)` | `r round(as.matrix(results_CNN2)[2], 3)` |

It is clear that the convolutional neural network does not do well at predicting the correct classification with the validation accuracy being the same as a naive prediction guess would be. The test set accuracy for CNN2, which performs only marginally better, is: `r round(as.matrix(result_test_CNN)[2], 3)` which is very poor predictive performance.

## [Discussion]{.underline}

Having presented the results a brief discussion of the performance of the models and which model and data would be recommended to use is now given.

As noted in the results section, none of the predictive models have very good classification accuracy with the best model having accuracy of around 57%. While this is not a very good percentage of classification accuracy, this is a significant improvement on the naive classification of guessing a president which equates to 25%.

The classification tree was one of the worst performing models with only around 27% classification accuracy. This could be somewhat expect as it can be seen as the most simple of all the predictive models presented here. Interestingly the convolutional neural network also performed very poorly which seems more unexpected. A limitation of the findings here is that more exploration of different kinds of convolutional neural networks with different hyperparameters was not done.

The boosting trees model gave a significant improvement in classification accuracy from the simple classification tree model and this showed how ensembling methods can be benefical for prediction and demostrate how the algorithm which learns from previous trees error can significantly improve results.

The neural networks proved to have the best classification error of all four kinds of models fit to this data. The best model and data input combination was the simplest neural network, using all words and TFIDF values instead of word counts. This would be the model that is recommended to use for this problem as it performed the best however it is not particularly accurate and other models and data formats should potentially be considered in future to improve the classification accuracy. In addition it is recommended that more validation analysis and tuning of various hyper parameters is carried out.

## [Conclusion]{.underline}

While the results of the models presented here are not particularly strong, the data exploration and model building process have demonstrated how one is able to manipulate and work with text data in order to build predictive models were text is used as inputs. In addition the process has given an overview of a number of techniques that can be used to fit predictive models, including Classification Trees, Boosted Trees, Neural Networks and Convolutional Neural Networks. The model that performed the best for this problem was a simple neural network that had a single hidden layer with 50 nodes. This model took as input a sentence of a speech and was able to predict which of four South African presidents said the sentence with 57% accuracy.

## [References]{.underline}

Allaire, J. and Chollet, F. 2023. \_keras: R Interface to 'Keras'\_. R package version 2.13.0.9000,
https://tensorflow.rstudio.com/.

Hastie, T., Tibshirani, R. and Friedman, J. 2008. The Elements of Statistical Learning. Springer.

Prasanna, P. and Rao, D. 2018. *Text classification using artificial neural networks.* International Journal of Engineering and Technology.

R Core Team. 2022. R: A language and environment for statistical computing. R Foundation for
Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.

Silge, J. and Robinson, D. 2016. Text Mining with R, A tidy approach. O'Reilly.

Silge, J. and Robinson, D. 2016. "tidytext: Text Mining and Analysis Using Tidy Data Principles in R."
\_JOSS\_, \*1\*(3). doi:10.21105/joss.00037 https://doi.org/10.21105/joss.00037.

Wickham, H. 2022. \_stringr: Simple, Consistent Wrappers for Common String Operations\_. R package version
 1.5.0, \<https://CRAN.R-project.org/package=stringr\>.

Wickham H., François R., Henry L., Müller K., Vaughan D. 2023. \_dplyr: A Grammar of Data Manipulation\_. R package version 1.1.3, https://CRAN.R-project.org/package=dplyr.

Zhou, C., Sun, C., Liu, Z. and Lau, F. 2015. *A C-LSTM Neural Network for Text Classification.* Department of Computer Science, University of Hong Kong.
