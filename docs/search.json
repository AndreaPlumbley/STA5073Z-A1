[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "This is a website for the submission of Assignment 1 for STA5073Z."
  },
  {
    "objectID": "Appendix.html",
    "href": "Appendix.html",
    "title": "Appendix",
    "section": "",
    "text": "This will contain the appendix of the report and maybe some code.\n\n2 + 2\n\n[1] 4"
  },
  {
    "objectID": "report_working.html#introduction",
    "href": "report_working.html#introduction",
    "title": "Assignment 1: Report",
    "section": "Introduction",
    "text": "Introduction\nThe aim of this assignment is to ‘predict the president’, to build predictive models that take in a sentence of text from a speech and predict which South African president said it. The data set given is a collection of 36 State of the Nation Addresses (SONA) in South Africa, delivered between 1994 and 2022. The assignment focuses on text mining and manipulation as well as predictive models, particularly focusing on neural networks and their classification performance. Before unpacking the data, a brief literature review is first given. Following this the data cleaning and exploration will be detailed and key features of the data discussed. The methods to format the data as well as the training, validation and test data splits will be outlined. The methods used to construct the different predictive models will then be detailed. Following this, results of the different predictive models will be presented and discussed and final conclusions made."
  },
  {
    "objectID": "report_working.html#literature-review",
    "href": "report_working.html#literature-review",
    "title": "Assignment 1: Report",
    "section": "Literature Review",
    "text": "Literature Review\nA brief summary of the literature on text classification techniques is now presented."
  },
  {
    "objectID": "report_working.html#data-cleaning-tokenization-and-exploration",
    "href": "report_working.html#data-cleaning-tokenization-and-exploration",
    "title": "Assignment 1: Report",
    "section": "Data Cleaning, Tokenization, and Exploration",
    "text": "Data Cleaning, Tokenization, and Exploration\nThe data that is used in this particular problem are the State of the Nation Addresses in South Africa from 1994 to 2022. There a 36 speeches given by 6 different presidents, namely Mandela, de Klerk, Mbkei, Zuma, Motlanthe and Ramaphosa. The initial steps in the solving the predict the president problem is to break the speeches up into their individual sentences and remove any unwanted characters, numbers or punctuation marks. The speeches were tokenized, broken up into smaller parts, using the unnest_tokens function in R. Each speech was split up into its sentences and the new data structure included the sentence along with the president who said it. For modelling purposes the sentences need to be only made up of their words so that when each sentence is tokenized into words, punctuation marks and numbers are not recognized as individual words. In order to do this the str_replace_all() function in the stringr package was used in order to remove any unwanted characters which were specified using a regular expression (regex). Number, commas, question marks and exclamation marks are some examples of the characters that were removed from analysis.\nSome exploration is required to check how imbalanced the classes are in terms of how many sentences are linked with each president. Table 1 below indicates the number of sentences associated with each president. It is clear that de Klerk and Motlanthe have much fewer associated sentences, this is expected as each of them only delivered one speech. Because of this large discrepancy in number of sentences, de Klerk and Motlanthe are removed from the analysis. The remaining four classes are still imbalanced with Mandela having the fewest sentences with 1665. To account for this all classes are made to have 1665 sentences and this is done by sampling without replacement 1665 sentences from the remaining three presidents: Mbeki, Zuma and Ramaphosa.\nTable 1: Number of sentences by each president.\n\n\n\nde Klerk\nMandela\nMbeki\nMotlanthe\nRamaphosa\nZuma\n\n\n\n\n97\n1665\n2419\n266\n2286\n2656\n\n\n\nHaving split the speeches up into sentences and removed the unwanted characters and balancing the number of sentences per president, each sentence could then be tokenized into its individual words. The sentences need to be broken down into words in order to create bag of words data structures which will be used in the predictive models.\n\n\n\nAgain the unnest_tokens() function is used to break the sentences up into words where each word is now associated with a president and sentence number in order to keep track of which words belong in which sentences and who said them.\nBecause many of the presidents all use common words throughout their speeches, these words need to be removed from the analysis because they do not help one to differentiate one president from another. These words are referred to as stop words and include words such as: the, and, if, them, etc. A stop_words dictionary exists which was used to remove these words from the analysis.\n\n\nJoining with `by = join_by(word)`\nJoining with `by = join_by(sentence_ID)`\n\n\nWe can consider the top words each presidents says as this will likely be used as one of the input data structures. Figure 1 displays the top 10 words said by each president. From this it is clear that there is significant overlap between presidents with words such as government, south and people being said by all presidents. There are however some words that are said many times by each president that do not appear in other presidents top 10 words. For example Zuma says compatriots many times and Ramaphosa refers frequently to the economy while others do not do so as frequently. These differences in the frequent words may be useful when deciding on the different data input structures to use in the predictive models."
  },
  {
    "objectID": "report_working.html#training-validation-and-test-splits",
    "href": "report_working.html#training-validation-and-test-splits",
    "title": "Assignment 1: Report",
    "section": "Training, Validation and Test Splits",
    "text": "Training, Validation and Test Splits\nHaving constructed the required data formats for the predictive model, the data must be split into training, validation and test sets before the predictive models are built. The split that will be used here is 60% training, 30% validation and 10% test data. The data splits are done grouping by president so that the classes of each president are balance in training , validation and test sets.\nThe seed was set to 2023 to ensure reproducibility. The training, validation and test sets were created by sampling from the sentence_ID variable and using this along with the anti_join() function to create the associated training, validation and test set vector of indices. For the different data formats, eg bag of words with counts versus TFIDF, the training data was created by selecting those rows where the sentence_ID variable matches those IDs in the training set vector of indices.\nThe training set will be used to build the different predictive models. The validation set will be used to select which hyper-parameters or model configuration to use. The test set will provide a final measure of performance of the selected model.\n\n\nJoining with `by = join_by(sentence_ID)`\nJoining with `by = join_by(sentence_ID)`"
  },
  {
    "objectID": "report_working.html#methods",
    "href": "report_working.html#methods",
    "title": "Assignment 1: Report",
    "section": "Methods",
    "text": "Methods\nHaving outlined the various data input structures that will be considered in the models as well as how the data has been split into its training, validation and testing classes, the predictive models that are used are now outlined and methods to implement them explained.\n\nClassification Tree\nThe first predictive model applied to the data is a simple classification tree. A classification tree is a decision tree that parititions the data on different variables in order to categorize each observation into a class. In this case a classification tree would take a sentence and based on the word counts or TFIDF counts, classify the sentence as being from one of the four presidents.\nA classification tree was fit using the rpart() function in R. Four models were fit, one on each of the four training data sets which are the full bag-of-words counts data, the full TFIDF data, the top 200 bag of words count data and the top 200 TFIDF data. The ‘method’ argument was set to ‘class’ because the response is categorical. The remaining arguments of the rpart() function were left at the default settings. For the classification tree model, no tuning was performed.\n\n\nRandom Forest\nThe second predictive model that was fit was a random forest model. This is an ensemble learning method and follows on from a simple classification tree. This method uses a collection of different classification trees in order to classify observations into its classes and takes an aggregate of the trees decisions.\nNEED TO EXPLAIN THIS MORE. ONLY INCLUDE IF IT WORKS\n\n\nFeed-Forward Neural Network\nA standard feed-forward neural network was fit to the different data types to predict the president who delivered the sentence. Here a number of different configurations of neural networks, with different numbers of layers and nodes were constructed and the validation set used to select the best model.\nNeed a sentence here to explain what a neural network is.\nThere are many architectures, or configurations, that can be done by varying the number of layers in the network, number of nodes per layers and other parameters such as learning rate and activation functions. The selection or ‘tuning’ of these parameters is done manually by fitting 4 different neural networks using the training data and selecting the optimal architecture based on the validation set performance. The differing configurations will be fit on the four different input data structures.\nWhat different configurations are done.\nThese feed forward neural networks were built and trained using the keras package in R.\nGo through some details of what keras does.\nStructure of response for keras neural nets - one hot encoding\n\n\nConvolutional Neural Network\nA more advanced neural network is also fit to the data to see if performance is improved. A convolutional neural network.\nExplain this once its actually been done.\nExplain this - is word embedding essential?? If so maybe we only fit with one kind of data input"
  },
  {
    "objectID": "report_working.html#results",
    "href": "report_working.html#results",
    "title": "Assignment 1: Report",
    "section": "Results",
    "text": "Results\nHaving outlined the methods and models implemented, the results of these models are now given.\n\nClassification Tree\n\n## Classification Tree Results:\n\n## CT 1 - All Words - BoW Counts\n\ntraining_all_BAG = cbind(Y_train_all_BAG, X_train_all_BAG)\nfit_all_BAG <- rpart(Y_train_all_BAG ~ ., training_all_BAG, method = 'class')\n\n#plot(fit_all_BAG, main = 'Full Classification Tree')\n#text(fit_all_BAG, use.n = TRUE, all = TRUE, cex=.8)\n\nfittedtrain <- predict(fit_all_BAG, type = 'class')\npredtrain <- table(training_all_BAG$Y_train_all_BAG, fittedtrain)\npredtrain\n\n           fittedtrain\n            Mandela Mbeki Ramaphosa Zuma\n  Mandela         0    31       961    0\n  Mbeki           0    77       934    2\n  Ramaphosa       0     1       982    0\n  Zuma            0     8       919   55\n\naccuracy_all_BAG_train = round(sum(diag(predtrain))/sum(predtrain), 3) # training accuracy\n\nfittedtest <- predict(fit_all_BAG, newdata = cbind(Y_test_all_BAG, X_test_all_BAG), type = 'class')\npredtest <- table(Y_test_all_BAG, fittedtest)\npredtest\n\n              fittedtest\nY_test_all_BAG Mandela Mbeki Ramaphosa Zuma\n     Mandela         0     7       162    0\n     Mbeki           0    15       151    0\n     Ramaphosa       0     0       160    0\n     Zuma            0     5       153    9\n\naccuracy_all_BAG_test = round(sum(diag(predtest))/sum(predtest), 3) # test accuracy \n\n\n## CT 2 - All Words - TFIDF\n\ntraining_all_TFIDF = cbind(Y_train_all_tfidf, X_train_all_tfidf)\nfit_all_TFIDF <- rpart(Y_train_all_tfidf ~ ., training_all_TFIDF, method = 'class')\n\n#plot(fit_all_BAG, main = 'Full Classification Tree')\n#text(fit_all_BAG, use.n = TRUE, all = TRUE, cex=.8)\n\nfittedtrain <- predict(fit_all_TFIDF, type = 'class')\npredtrain <- table(training_all_TFIDF$Y_train_all_tfidf, fittedtrain)\npredtrain\n\n           fittedtrain\n            Mandela Mbeki Ramaphosa Zuma\n  Mandela         0    31       961    0\n  Mbeki           0    77       934    2\n  Ramaphosa       0     1       982    0\n  Zuma            0     8       919   55\n\naccuracy_all_TFIDF_train = round(sum(diag(predtrain))/sum(predtrain), 3) # training accuracy\n\nfittedtest <- predict(fit_all_TFIDF, newdata = cbind(Y_test_all_tfidf, X_test_all_tfidf), type = 'class')\npredtest <- table(Y_test_all_tfidf, fittedtest)\npredtest\n\n                fittedtest\nY_test_all_tfidf Mandela Mbeki Ramaphosa Zuma\n       Mandela         0     7       162    0\n       Mbeki           0    15       151    0\n       Ramaphosa       0     0       160    0\n       Zuma            0     5       153    9\n\naccuracy_all_TFIDF_test = round(sum(diag(predtest))/sum(predtest), 3) # test accuracy \n\n## CT 3 - 200 Words - BoW Counts\n\ntraining_200_BAG = cbind(Y_train_200_BAG, X_train_200_BAG)\nfit_200_BAG <- rpart(training_200_BAG$Y_train_200_BAG ~ ., data =  training_200_BAG, method = 'class')\n\nplot(fit_200_BAG, main = 'Full Classification Tree')\ntext(fit_200_BAG, use.n = TRUE, all = TRUE, cex=.8)\n\n\n\nfittedtrain <- predict(fit_200_BAG, newdata = cbind(Y_train_200_BAG, X_train_200_BAG), type = 'class')\npredtrain <- table(training_200_BAG$Y_train_200_BAG, fittedtrain)\npredtrain\n\n           fittedtrain\n            Mandela Mbeki Ramaphosa Zuma\n  Mandela         0    31       900    0\n  Mbeki           0    77       893    2\n  Ramaphosa       0     1       934    0\n  Zuma            0     8       848   55\n\naccuracy_200_BAG_train = round(sum(diag(predtrain))/sum(predtrain), 3) # training accuracy\n\nfittedtest <- predict(fit_200_BAG, newdata = cbind(Y_test_200_BAG, X_test_200_BAG), type = 'class')\npredtest <- table(Y_test_200_BAG, fittedtest)\npredtest\n\n              fittedtest\nY_test_200_BAG Mandela Mbeki Ramaphosa Zuma\n     Mandela         0     7       156    0\n     Mbeki           0    15       141    0\n     Ramaphosa       0     0       150    0\n     Zuma            0     5       145    9\n\naccuracy_200_BAG_test = round(sum(diag(predtest))/sum(predtest), 3) # test accuracy \n\n\n## CT 4 - 200 Words - TFIDF\n\ntraining_200_TFIDF = cbind(Y_train_200_tfidf, X_train_200_tfidf)\nfit_200_TFIDF <- rpart(Y_train_200_tfidf ~ ., training_200_TFIDF, method = 'class')\n\nplot(fit_200_TFIDF, main = 'Full Classification Tree')\ntext(fit_200_TFIDF, use.n = TRUE, all = TRUE, cex=.8)\n\n\n\nfittedtrain <- predict(fit_200_TFIDF, newdata = cbind(Y_train_200_tfidf, X_train_200_tfidf), type = 'class')\npredtrain <- table(training_200_TFIDF$Y_train_200_tfidf, fittedtrain)\npredtrain\n\n           fittedtrain\n            Mandela Mbeki Ramaphosa Zuma\n  Mandela         0    31       900    0\n  Mbeki           0    77       893    2\n  Ramaphosa       0     1       934    0\n  Zuma            0     8       848   55\n\naccuracy_200_TFIDF_train = round(sum(diag(predtrain))/sum(predtrain), 3) # training accuracy\n\nfittedtest <- predict(fit_200_TFIDF, newdata = cbind(Y_test_all_tfidf, X_test_all_tfidf), type = 'class')\npredtest <- table(Y_test_200_tfidf, fittedtest)\npredtest\n\n                fittedtest\nY_test_200_tfidf Mandela Mbeki Ramaphosa Zuma\n       Mandela         0     7       156    0\n       Mbeki           0    14       142    0\n       Ramaphosa       0     1       149    0\n       Zuma            0     4       148    7\n\naccuracy_200_TFIDF_test = round(sum(diag(predtest))/sum(predtest), 3) # test accuracy \n\nTable 2: Classification Accuracy of Simple Classification Tree\n\n\n\n\n\n\n\n\n\n\nData Input Structure.\nAll BOW Counts\nAll TFIDF\n200 Words Counts\n200 Words TFIDF\n\n\n\n\nTraining Classification Accuracy\n0.281\n0.281\n0.284\n0.284\n\n\nTest Classification Accuracy\n0.278\n0.278\n0.277\n0.271\n\n\n\n\n\n\n\n\n\n\n\n\nNow lets try a cnn\n\nmax_features <- 1000        # choose max_features most popular words\ntokenizer = text_tokenizer(num_words = max_features)\nfit_text_tokenizer(tokenizer, sona_sentences_FINAL$sentences[4])\n\n\nsona_sentences_FINAL$sentences[4]\ntts <- texts_to_sequences(tokenizer, sona_sentences_FINAL$sentences[4])\ntts\n\ntokenizer$word_index[tts[[1]]]\n\nsequences = tokenizer$texts_to_sequences(sona_sentences_FINAL$sentences)\n\ntrain_row = train_ids$sentence_ID\nval_row = val_ids$sentence_ID\n\ny = as.factor(sona_sentences_FINAL$president_name)\ny = as.integer(y)\n\ntrain <- list()\nval <- list()\ntrain$x <- sequences[train_row]\nval$x <-  sequences[val_row]\n\ntrain$y <- y[train_row]\nval$y <-  y[val_row]\n\nhist(unlist(lapply(sequences, length)), main = \"Sequence length after tokenization\")\n\nmaxlen <- 15            \nx_train <- train$x %>% pad_sequences(maxlen = maxlen)\nx_test <- val$x %>% pad_sequences(maxlen = maxlen)\n\nmodel <- keras_model_sequential() %>% \n  layer_embedding(max_features, output_dim = 10, input_length = maxlen) %>%\n  layer_dropout(0.2) %>%\n  layer_flatten() %>%\n  layer_dense(100, activation = \"relu\") %>%\n  layer_dense(4, activation = \"softmax\")\n\nsummary(model)\n\nmodel %>% compile(\n  loss = \"categorical_crossentropy\",\n  optimizer = \"adam\",\n  metrics = \"accuracy\"\n)\n\nhistory <- model %>% fit(x_train,Y_train,\n    batch_size = 600, epochs = 30, verbose = 0)\nplot(history)\n\n\nembedding_dims <- 10\nmodel <- keras_model_sequential() %>% \n  layer_embedding(max_features, output_dim = embedding_dims, input_length = maxlen) %>%\n  layer_dropout(0.2) %>%\n  layer_conv_1d(filters = 64, kernel_size = 8, activation = \"relu\") %>%\n  layer_max_pooling_1d(pool_size = 2) %>%\n  layer_flatten() %>%\n  layer_dense(300, activation = \"relu\") %>%\n  layer_dense(4, activation = \"softmax\")\n\nsummary(model)\n\nmodel %>% compile(\n  loss = \"categorical_crossentropy\",\n  optimizer = \"adam\",\n  metrics = \"accuracy\"\n)\n\nhistory <- model %>% fit(x_train,Y_train,\n    batch_size = 600, epochs = 30, verbose = 0)\nplot(history)\n\nresults <- model %>% evaluate(x_test, Y_val, batch_size=600, verbose = 2)\n\nLets try random forest with tfidf\n\nlibrary(ranger)\n\nX_tfidf = tfidf[,3:ncol(tfidf)]\nX_tfidf_train = X_tfidf[train_ids$sentence_ID,]\n\n\nY_train_forest = (tfidf[train_ids$sentence_ID,2])\n\n# Assuming your response variable is in 'y' and your bag of words matrix is in 'X'\nmodel <- ranger(tfidf[train_ids$sentence_ID,2], tfidf[train_ids$sentence_ID,-c(2)], num.trees = 100, importance = \"permutation\")\n\n\n\nRandom Forest\n\n\nFeed-Forward Neural Network\n\n\nConvolutional Neural Network\n\n\nOverall Test set results"
  },
  {
    "objectID": "report_working.html#discussion",
    "href": "report_working.html#discussion",
    "title": "Assignment 1: Report",
    "section": "Discussion",
    "text": "Discussion\n\nWhich type of predictive model had most satisfactory results\n\n\nWhich input data structure seemed to be best"
  },
  {
    "objectID": "report_working.html#data-input-structures",
    "href": "report_working.html#data-input-structures",
    "title": "Assignment 1: Report",
    "section": "Data Input Structures",
    "text": "Data Input Structures\nThe text data needs to be put into a specific format for the predictive models for which we use a bag of words with counts and TFIDF values. In a bag of words model a document (in this case working with sentences) is represented by the set of words used in it (REFERENCE). In this case each row of the bag of words data frame represents a sentence where the columns are the possible words used. The number of columns is very large because all words used by all presidents, excluding the stop words or other words ‘cleaned away’, are represented on the columns. The matrix is relatively sparse in that for each row (sentence) only a couple columns have values due to only a few words being used in each sentence. The values are counts of how many times each word was used in each sentence. This format is the first type of input data structure use in the predictive models.\nThe second input structure used in the predictive models is similar in format to the above except that instead of the values being counts of each word in each sentence, the value is a TFIDF value. A TFIDF (Term-frequency-inverse-document-frequency) value is a way of distinguishing which words are more or less ‘important’ in the delivered speeches. This value is made up of two calculations:\n\nThe term frequency (tf) which is the number of times a specific terms appears in a document, divided by the number of terms in that document.\nThe inverse document frequency (idf) which is a measure of how many documents contain the specific terms and is calculated as the log of the total number of documents divided by the number of documents containing the specific word.\n\nThe TFIDF value for a word is the product of the tf and idf measures. This data structure thus contains all the words as columns, the sentences as the rows (made up of different words) and the values in each cell are the TFIDF values. This data input structure may be more beneficial than the bag of words counts because it gives more information in terms of the relative importance or significance of the different words used in different sentences.\nAn additional way of varying the input structure which also helps reduce the dimensions of input data is to use only the top 200 most frequent words used by each president. This will help see if using more frequently used words helps better identify which president is speaking. As seen in the initial data exploration of each presidents top 10 words, there may however be significant overlap in each presidents most frequently used words and so this may not be an effective way of improving the predictions. However there maybe be unique words that each president says often that help differentiate them from one another. The top 200 words format is similar to the bag of words format in that the columns are words that make up each presidents most repeated words and the rows represent the sentences. Both counts and TFIDF scores will be used in the 200 top words data input format.\nThe above differing formats result in four different types of format of input data, two are what will be referred to as ‘all words’ formats which contain all the words except for those removed in cleaning. The one ‘all words’ format uses counts of word so form a bag of words model and the other ‘all words’ format uses TFIDF scores. The two ‘200 words’ input formats refer to those inputs that use only the top 200 most repeated words from each president. Again, one of these uses word counts and the other uses TFIDF values.\nWord embedding???\n\n\n\n\n\nJoining with `by = join_by(sentence_ID)`\nJoining with `by = join_by(sentence_ID)`"
  }
]