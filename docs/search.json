[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "This is a website for the submission of Assignment 1 for STA5073Z."
  },
  {
    "objectID": "Appendix.html",
    "href": "Appendix.html",
    "title": "Appendix",
    "section": "",
    "text": "This will contain the appendix of the report and maybe some code.\n\n2 + 2\n\n[1] 4"
  },
  {
    "objectID": "report_working.html#introduction",
    "href": "report_working.html#introduction",
    "title": "Assignment 1: Report",
    "section": "Introduction",
    "text": "Introduction\nThe aim of this assignment is to ‘predict the president’. The data set given is a collection of 36 State of the Nation speeches in South Africa, delivered between 1994 and 2022. The aim of the assignment is to ‘predict the president’ by building a model that takes in as a input a particular sentences and returns a prediction of which president said the sentence. The assignment thus focuses on text mining and manipulation as well as predictive models, particularly focusing on neural networks. A brief literature review is first given. Following this the data cleaning and exploration will be detailed and key features of the data discussed. The methods used to construct the different predictive models as well as format the data will then outline. Results of the different predictive models will be presented and discussed and final conclusions made."
  },
  {
    "objectID": "report_working.html#literature-review",
    "href": "report_working.html#literature-review",
    "title": "Assignment 1: Report",
    "section": "Literature Review",
    "text": "Literature Review"
  },
  {
    "objectID": "report_working.html#data-cleaning-tokenization-and-exploration",
    "href": "report_working.html#data-cleaning-tokenization-and-exploration",
    "title": "Assignment 1: Report",
    "section": "Data Cleaning, Tokenization, and Exploration",
    "text": "Data Cleaning, Tokenization, and Exploration\nThe data that is used in this particular problem are the State of the Nation speeches in South Africa from 1994 to 2022. There a 36 speeches given by 6 different presidents, namely Mandela, de Klerk, Mbkei, Zuma, Motlanthe and Ramaphosa.. The initial steps in the solving the predict the president problem is to break the speeches up into their indivudal sentences and remove any unwanted characters, numbers or punctuation marks. The speeches were tokenized, broken up into smaller parts, using the unnest_tokens function in R. Each speech was split up into its sentences and the new data structure included the sentence along with the president who said it. For modelling purposes the sentences need to be only made up of their words so that when each sentence is tokenized into words, punctuation marks and numbers are not recognized as individual words. In order to do this the str_replace_all() function in the stringr package was used in order to remove any unwanted characters which were specified using a regual expression (regex). Number, commas, question marks and exclamation marks are some examples of the characters that were removed from analysis.\nSome exploration is reuired to check how imbalanced these classes are in terms of how many sentences are linked with each president. Table 1 below indicates the number of sentences associated with each president. It is clear that de Klerk and Motlanthe have much fewer associated sentences, this is expected as each of them only delivered one speech. Because of this large discrepancy in number of sentences, de Klerk and Motlanthe are removed from the analysis. The remaining four classes are still imbalanced with Mandela having the fewest sentences with 1665. To account for this all classes are made to have 1665 sentences and this is done by sampling without replacement 1665 sentences from the remaining three presidents: Mbeki, Zuma and Ramaphosa.\nTable 1: Number of sentences by each president.\n\n\n\nde Klerk\nMandela\nMbeki\nMotlanthe\nRamaphosa\nZuma\n\n\n\n\n97\n1665\n2419\n266\n2286\n2656\n\n\n\nHaving split the speeches up into sentences and removed the unwanted characters and balancing the number of sentences per president, each sentence could then be tokenized into its individual words. The sentences need to be broken down into words in order to create bag of words data structures which will be used in the predictive models.\n\n\n\nAgain the unnest_tokens() function is used to break the sentences up into words where each word is now associated with a president and sentence number in order to keep track of which words belong in which sentences and who said them.\nStop words are then removed.\nThen bag of words format created\nThen tfidf format created\nWhat about top 200 words.\n\n\nJoining with `by = join_by(word)`\nJoining with `by = join_by(sentence_ID)`\n\n\nWe now have our bag of words each associated with a sentence ID and president name. This bag of words is word counts we also need to set up tfidf.\nTFIDF\n\n\nJoining with `by = join_by(sentence_ID)`\n\n\nNow have bag of words with counts and with TFIDF\nNeed to add one where we only consider top 500 words said by each president.\n\n\nJoining with `by = join_by(sentence_ID)`\nJoining with `by = join_by(sentence_ID)`"
  },
  {
    "objectID": "report_working.html#training-validation-and-test-splits",
    "href": "report_working.html#training-validation-and-test-splits",
    "title": "Assignment 1: Report",
    "section": "Training, Validation and Test splits",
    "text": "Training, Validation and Test splits\nNow need to create training, validation and test splits with the data - by president so that classes are balanced.\nTraining = 60% Validation = 30% Test = 10%\n\n\nJoining with `by = join_by(sentence_ID)`\nJoining with `by = join_by(sentence_ID)`\n\n\nNow that we have got the ids for the train, validation and test set we can start fitting some models. Starting with a simple classfication tree and using count bag of words.\n\n\n\n\n\n           fittedtrain\n            Mandela Mbeki Ramaphosa Zuma\n  Mandela         0    31       961    0\n  Mbeki           0    77       934    2\n  Ramaphosa       0     1       982    0\n  Zuma            0     8       919   55\n\n\n[1] 0.281\n\n\n           fittedtest\nY_val       Mandela Mbeki Ramaphosa Zuma\n  Mandela         0    22       473    0\n  Mbeki           0    27       449    0\n  Ramaphosa       0     0       510    0\n  Zuma            0     7       468   29\n\n\n[1] 0.285\n\n\nClearly the classification tree is not doing very well on word counts bag of words model. What about tfidf\n\n\n\n\n\n           fittedtrain\n            Mandela Mbeki Ramaphosa Zuma\n  Mandela         0    31       961    0\n  Mbeki           0    77       934    2\n  Ramaphosa       0     1       982    0\n  Zuma            0     8       919   55\n\n\n[1] 0.281\n\n\n           fittedtest\nY_val       Mandela Mbeki Ramaphosa Zuma\n  Mandela         0    22       473    0\n  Mbeki           0    27       449    0\n  Ramaphosa       0     0       510    0\n  Zuma            0     7       468   29\n\n\n[1] 0.285\n\n\n\n\n\nClearly the classification tree is not doing well so now we try a neural network.\n\nlibrary(keras)\n      # This may be ruining the run\n      # noticed that after running this, your \n      # Dimensions don't match up\n\n#zero_columns_train <- apply(X_train, 2, function(col) all(col == 0))\n#X_train = X_train[,!zero_columns_train]\n#X_val = X_val[,!zero_columns_train]\n#X_test = X_test[,!zero_columns_train]\n\ntraining_data_tfidf = tfidf %>%\n                      right_join(train_ids, by='sentence_ID') %>%\n                      select(-sentence_ID)\nX_train = training_data_tfidf[,2:ncol(training_data_tfidf)]\nY_train = as.factor(training_data_tfidf$president_name)\n\n\nval_data_tfidf = tfidf %>%\n                  right_join(val_ids, by='sentence_ID') %>%\n                  select(-sentence_ID)\nX_val = val_data_tfidf[,2:ncol(val_data_tfidf)]\nY_val = as.factor(val_data_tfidf$president_name)\n\ntest_data_tfidf = tfidf %>%\n                  right_join(test_ids, by='sentence_ID') %>%\n                  select(-sentence_ID)\nX_test = test_data_tfidf[,2:ncol(test_data_tfidf)]\nY_test = as.factor(test_data_tfidf$president_name)\n  # Added this in\nX_train <- as.matrix(X_train)\nX_val <- as.matrix(X_val)\n  # This is how keras takes in one-hot encoding\nY_train <- to_categorical(as.integer(unlist(Y_train)) - 1)\nY_val  <- to_categorical(as.integer(unlist(Y_val)) - 1)\n\nmodel <- keras_model_sequential() %>%\n  layer_dense(units = 100, input_shape = c(9085), activation = \"relu\") %>%\n  layer_dense(units = 4, activation = \"softmax\")\n\nsummary(model)\n\nModel: \"sequential\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n dense_1 (Dense)                    (None, 100)                     908600      \n dense (Dense)                      (None, 4)                       404         \n================================================================================\nTotal params: 909004 (3.47 MB)\nTrainable params: 909004 (3.47 MB)\nNon-trainable params: 0 (0.00 Byte)\n________________________________________________________________________________\n\nmodel %>% compile(\n  loss = \"categorical_crossentropy\",\n  optimizer = \"adam\",\n  metrics = \"accuracy\"\n)\n\nhistory <- model %>% fit(X_train, Y_train, epochs = 40, batch_size = 100, verbose = 0) \nplot(history)\n\n\n\nresults <- model %>% evaluate(X_val, Y_val, batch_size=128, verbose = 2)\n\n16/16 - 0s - loss: 1.9720 - accuracy: 0.5446 - 74ms/epoch - 5ms/step\n\n\nNow lets try a cnn\n\n\n[1] \"it must be electrified and supplied with water\"\n\n\n[[1]]\n[1] 1 2 3 4 5 6 7 8\n\n\n$it\n[1] 1\n\n$must\n[1] 2\n\n$be\n[1] 3\n\n$electrified\n[1] 4\n\n$and\n[1] 5\n\n$supplied\n[1] 6\n\n$with\n[1] 7\n\n$water\n[1] 8\n\n\n\n\n\nModel: \"sequential_1\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n embedding (Embedding)              (None, 15, 10)                  10000       \n dropout (Dropout)                  (None, 15, 10)                  0           \n flatten (Flatten)                  (None, 150)                     0           \n dense_3 (Dense)                    (None, 100)                     15100       \n dense_2 (Dense)                    (None, 4)                       404         \n================================================================================\nTotal params: 25504 (99.62 KB)\nTrainable params: 25504 (99.62 KB)\nNon-trainable params: 0 (0.00 Byte)\n________________________________________________________________________________\n\n\n\n\n\n\nembedding_dims <- 10\nmodel <- keras_model_sequential() %>% \n  layer_embedding(max_features, output_dim = embedding_dims, input_length = maxlen) %>%\n  layer_dropout(0.2) %>%\n  layer_conv_1d(filters = 64, kernel_size = 8, activation = \"relu\") %>%\n  layer_max_pooling_1d(pool_size = 2) %>%\n  layer_flatten() %>%\n  layer_dense(300, activation = \"relu\") %>%\n  layer_dense(4, activation = \"softmax\")\n\nsummary(model)\n\nModel: \"sequential_2\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n embedding_1 (Embedding)            (None, 15, 10)                  10000       \n dropout_1 (Dropout)                (None, 15, 10)                  0           \n conv1d (Conv1D)                    (None, 8, 64)                   5184        \n max_pooling1d (MaxPooling1D)       (None, 4, 64)                   0           \n flatten_1 (Flatten)                (None, 256)                     0           \n dense_5 (Dense)                    (None, 300)                     77100       \n dense_4 (Dense)                    (None, 4)                       1204        \n================================================================================\nTotal params: 93488 (365.19 KB)\nTrainable params: 93488 (365.19 KB)\nNon-trainable params: 0 (0.00 Byte)\n________________________________________________________________________________\n\nmodel %>% compile(\n  loss = \"categorical_crossentropy\",\n  optimizer = \"adam\",\n  metrics = \"accuracy\"\n)\n\nhistory <- model %>% fit(x_train,Y_train,\n    batch_size = 600, epochs = 30, verbose = 0)\nplot(history)\n\n\n\nresults <- model %>% evaluate(x_test, Y_val, batch_size=600, verbose = 2)\n\n4/4 - 0s - loss: 1.3973 - accuracy: 0.2413 - 63ms/epoch - 16ms/step\n\n\nLets try random forest with tfidf\n\nlibrary(ranger)\n\nX_tfidf = tfidf[,3:ncol(tfidf)]\nX_tfidf_train = X_tfidf[train_ids$sentence_ID,]\n\n\nY_train_forest = (tfidf[train_ids$sentence_ID,2])\n\n# Assuming your response variable is in 'y' and your bag of words matrix is in 'X'\nmodel <- ranger(tfidf[train_ids$sentence_ID,2], tfidf[train_ids$sentence_ID,-c(2)], num.trees = 100, importance = \"permutation\")\n\nWhat about multinomial logistic regression?"
  }
]